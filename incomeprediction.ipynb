{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEBDI Competition Submission: Lejvi Dalani & Hasan Tosun\n",
    "## Estimating Income From Worker Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the second part of the MEBDI competition. The goal of the competition was to estimate wage income using variables from the Current Population Survey (CPS), and improve Root-Mean-Square Error relative to the Mincer Regression model of income. In the second part of the competition, all variables in CPS  were allowed, as long as they were not directly related to realized income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import xgboost as xgb #requires pip install xgboost\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "from vecstack import stacking #pip install vecstack\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import catboost\n",
    "\n",
    "from catboost import CatBoostRegressor \n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_random_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The data\n",
    "\n",
    "The data came from IPUMS CPS 2012. We provide a list variables in `var_list.csv` with the variable names and whether they are used in any part of the project, and a description of the 125 variables used in `Variables_Dalani_Tosun.xslx`. Please refer to the submission report `Mebdi_Dalani_Tosun.pdf` for a detailed description of the data selection process. All files mentioned on this script can be found in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"data_part2.feather\")\n",
    "# df = pd.read_feather(\"data_part2.feather\")\n",
    "var_list = pd.read_csv(\"var_list.csv\")\n",
    "\n",
    "vars_dummy = var_list[var_list.decision == 'dummy']['var'].tolist()\n",
    "vars_numeric = var_list[var_list.decision == 'numeric']['var'].tolist()\n",
    "vars_entityembed = var_list[var_list.decision == 'entityembed']['var'].tolist()\n",
    "vars_necessary = var_list[var_list.decision == 'necessary']['var'].tolist()\n",
    "vars_drop = var_list[var_list.decision == 'drop']['var'].tolist()\n",
    "\n",
    "vars_to_use = vars_dummy + vars_numeric + vars_entityembed + vars_necessary \n",
    "\n",
    "df = df[vars_to_use]\n",
    "\n",
    "df.loc[:,'y'] = np.log(df.incwage)\n",
    "\n",
    "Train = df[df.type == \"train\"].drop(vars_necessary, axis = 1).reset_index(drop=True)\n",
    "Test = df[df.type == \"test\"].drop(vars_necessary, axis = 1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "Index_train = Train.index.to_numpy()\n",
    "Index_test = Test.index.to_numpy()\n",
    "\n",
    "Records_train = Train.to_records()\n",
    "Records_test = Test.to_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entity Embedding \n",
    "\n",
    "Variables occ (occupation) and occly (occupation last year) showed up as important features across\n",
    "multiple feature selection algorithms we tried. However, both features had high dimensionality (more\n",
    "than 500 categories in each), with very few observations for some categories in some of the samples.\n",
    "Given how small our sample size was, we decided to reduce the dimensionality of the two variables using *entity embedding*, using a Neural Network. \n",
    "\n",
    "This allowed us to reduce the dimensionality of each of the occupation variables\n",
    "to 23 each. The idea of entity embeddings is to create vectors in a smaller subspace (in this case in $R^{23}$),\n",
    "instead of the sparse $R^{500+}$ space needed by dummy encoding. Each vector in $R^{23}$ corresponds to one\n",
    "\"unique\" occupation. As a consequence, occ and occly were mapped into 46 columns. These 46 columns\n",
    "were merged into our dataframe, and the raw occ and occly variables were dropped, in order to avoid\n",
    "overfitting, since our train data had already seen the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping Data for Entity Embedding\n",
      "Running Neural Network on entity embedding for occ and occly\n",
      "Entity embedding for occ and occly done\n"
     ]
    }
   ],
   "source": [
    "print('Prepping Data for Entity Embedding')\n",
    "\n",
    "features = vars_entityembed\n",
    "\n",
    "#######################################################################\n",
    "# There are categorical values that may show up only in test. So for now, need to join Train and Test Data,\n",
    "# so as to create one hot encoding for all categories, and then right after separate.\n",
    "\n",
    "# Create var to unmerge on later\n",
    "Train.loc[:,'placeholder'] = 1\n",
    "Test.loc[:,'placeholder'] = -1\n",
    "\n",
    "# Merge\n",
    "DF_embed = pd.concat([Train, Test])\n",
    "\n",
    "DF_embed = DF_embed[features+['y']+['placeholder']]\n",
    "\n",
    "# Impute values for occ and occly that show up only in train based on crosswalk\n",
    "DF_embed['occ'].replace(1660, 1610, inplace = True)\n",
    "DF_embed['occ'].replace(5200, 5165, inplace = True)\n",
    "DF_embed['occ'].replace(7440, 7430, inplace = True)\n",
    "DF_embed['occ'].replace(7550, 7560, inplace = True)\n",
    "DF_embed['occ'].replace(8840, 8830, inplace = True)\n",
    "DF_embed['occ'].replace(8850, 8830, inplace = True)\n",
    "DF_embed['occ'].replace(8860, 8830, inplace = True)\n",
    "\n",
    "\n",
    "DF_embed['occly'].replace(1660, 1610, inplace = True)\n",
    "DF_embed['occly'].replace(5200, 5165, inplace = True)\n",
    "DF_embed['occly'].replace(5210, 5165, inplace = True)\n",
    "DF_embed['occly'].replace(7550, 7560, inplace = True)\n",
    "DF_embed['occly'].replace(8840, 8830, inplace = True)\n",
    "\n",
    "#Label Encode occ and occly: Turning everything into numbers basically    \n",
    "for col in features:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    DF_embed.loc[:,col] = encoder.fit_transform(DF_embed[col].astype(str).values)\n",
    "\n",
    "DF_embed = DF_embed[features +['placeholder','y']] \n",
    "    \n",
    "Placeholder = DF_embed['placeholder']\n",
    "Train_embed = DF_embed[DF_embed.placeholder == 1].drop(\"placeholder\", axis = 1).reset_index()\n",
    "Test_embed = DF_embed[DF_embed.placeholder == -1].drop(\"placeholder\", axis = 1).reset_index()\n",
    "\n",
    "\n",
    "## Create model\n",
    "\n",
    "def create_model(df, categorical_columns, num_features_param):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for c in categorical_columns:\n",
    "        num_unique_vals = int(df[c].nunique())\n",
    "        embed_dim = int(min(np.sqrt(num_unique_vals) + 8, num_features_param)) # embed dimension\n",
    "        inpt = layers.Input(shape=(1, ))\n",
    "        outt = layers.Embedding(num_unique_vals, embed_dim, name = c)(inpt) \n",
    "        # Can apply dropout here to introduce some regularization\n",
    "        outt = layers.Reshape(target_shape = (embed_dim, ))(outt)\n",
    "        inputs.append(inpt)\n",
    "        outputs.append(outt)\n",
    "    X = layers.Concatenate()(outputs)\n",
    "    X = layers.Dense(2000, activation = 'relu')(X)\n",
    "    X = layers.Dense(10, activation = 'relu')(X)\n",
    "#     X = layers.Dropout(.2)(X)\n",
    "    y = layers.Dense(1, activation ='linear')(X)\n",
    "    model = Model(inputs = inputs, outputs = y)\n",
    "    return model\n",
    "\n",
    "print('Running Neural Network on entity embedding for occ and occly')\n",
    "# Run and compile model\n",
    "min_features = 23 \n",
    "model = create_model(Train_embed, features, min_features)\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.9999,                                     epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer = adam, loss = 'mean_squared_error', metrics=['mse', 'mae'])\n",
    "model.fit([Train_embed.loc[:, c].values for c in features], Train_embed.y.values,epochs=40, batch_size=4500, verbose = 0)\n",
    "\n",
    "\n",
    "model.evaluate([Test_embed.loc[:, c].values for c in features], Test_embed.y.values, batch_size=300, verbose = 0)\n",
    "result = model.predict([Test_embed.loc[:, c].values for c in features])\n",
    "rmse = np.sqrt(np.mean((result-Test_embed.y.values.reshape(-1, 1))**2))\n",
    "\n",
    "\n",
    "#### Get entity embeddings:\n",
    "Occ_emb   = pd.get_dummies(DF_embed.occ) @ model.get_layer('occ').get_weights()[0]\n",
    "Occly_emb = pd.get_dummies(DF_embed.occly) @ model.get_layer('occly').get_weights()[0]\n",
    "\n",
    "Occ_col_list = ['occ' + str(x) for x in range(1,24)]\n",
    "Occly_col_list = ['occly' + str(x) for x in range(1,24)]\n",
    "\n",
    "\n",
    "Occ_emb.columns = Occ_col_list\n",
    "Occly_emb.columns = Occly_col_list\n",
    "\n",
    "\n",
    "X_embed   = pd.concat([Occ_emb, Occly_emb], axis = 1) \n",
    "X_embed   = pd.concat([X_embed, Placeholder], axis = 1)\n",
    "\n",
    "# Store the embeddings\n",
    "X_embed_train = X_embed[X_embed.placeholder == 1].drop(\"placeholder\", axis = 1)\n",
    "X_embed_test  = X_embed[X_embed.placeholder == -1].drop(\"placeholder\", axis = 1)\n",
    "\n",
    "print('Entity embedding for occ and occly done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Label Encoding and One-Hot Encoding\n",
    "\n",
    "We decided to perform two types of encoding on the remaining categorical variables in our data:\n",
    "LabelEncoding (randomly assigning numbers to each category within a variable) and OneHotEncoding\n",
    "(creating dummies for each variable). These encodings were performed uniformly on all categorical\n",
    "variables in order to create three dataframes: $DF_{enc}$, $DF_{onehot}^1$ and $DF_{onehot}^2$, where $DF_{onehot}^1$ and $DF_{onehot}^2$ are randomly column partitioned (of equal size) from $DF_{onehot}$, a dataframe resulting from\n",
    "OneHot encoding all of the categorical variables. The partitions will become useful in the discussion\n",
    "that follows regarding the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping Data for Label Encoding\n",
      "Prepping Data for One Hot Encoding\n"
     ]
    }
   ],
   "source": [
    "#################### Label Encoded Section ########################\n",
    "print('Prepping Data for Label Encoding')\n",
    "\n",
    "\n",
    "vars_all_encoded = vars_dummy + vars_numeric \n",
    "vars_dummy_encoded = vars_dummy  \n",
    "    \n",
    "\n",
    "# Merge\n",
    "DF = pd.concat([Train, Test])\n",
    "\n",
    "DF = DF[vars_all_encoded +['placeholder','y']]\n",
    "\n",
    "for col in vars_dummy_encoded:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    DF.loc[:,col] = encoder.fit_transform(DF[col].astype(str).values)\n",
    "    \n",
    "Train_encoded = DF[DF.placeholder == 1].drop(\"placeholder\", axis = 1)\n",
    "Test_encoded = DF[DF.placeholder == -1].drop(\"placeholder\", axis = 1)\n",
    "\n",
    "Train_encoded = pd.concat([Train_encoded, X_embed_train], axis = 1)\n",
    "Test_encoded = pd.concat([Test_encoded, X_embed_test], axis = 1)\n",
    "\n",
    "X_train_encoded = Train_encoded.drop(\"y\", axis = 1).to_numpy()\n",
    "y_train_encoded = Train_encoded[\"y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_test_encoded = Test_encoded.drop(\"y\", axis = 1).to_numpy()\n",
    "y_test_encoded = Test_encoded[\"y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#################### One Hot Section ########################\n",
    "print('Prepping Data for One Hot Encoding')\n",
    "\n",
    "# Variable list for label encoded algorithms\n",
    "\n",
    "vars_all_onehot = vars_dummy + vars_numeric \n",
    "vars_dummy_onehot = vars_dummy\n",
    "\n",
    "\n",
    "# Merge\n",
    "DF = pd.concat([Train, Test])\n",
    "\n",
    "DF = DF[vars_all_onehot +['placeholder','y']]\n",
    "\n",
    "#One hot encode all categorical variables    \n",
    "DF_dummies = pd.get_dummies(data=DF, columns=vars_dummy_onehot)\n",
    "\n",
    "    \n",
    "Train_onehot = DF_dummies[DF_dummies.placeholder == 1].drop(\"placeholder\", axis = 1)\n",
    "Test_onehot = DF_dummies[DF_dummies.placeholder == -1].drop(\"placeholder\", axis = 1)\n",
    "\n",
    "# Merge in embedded occ and occly\n",
    "Train_onehot = pd.concat([Train_onehot, X_embed_train], axis = 1)\n",
    "Test_onehot = pd.concat([Test_onehot, X_embed_test], axis = 1)\n",
    "\n",
    "X_train_onehot = Train_onehot.drop(\"y\", axis = 1).to_numpy()\n",
    "y_train_onehot = Train_onehot[\"y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_test_onehot = Test_onehot.drop(\"y\", axis = 1).to_numpy()\n",
    "y_test_onehot = Test_onehot[\"y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_train = y_train_onehot # also equal to y_train_encoded\n",
    "y_test = y_test_onehot # also equal to y_test_encoded\n",
    "\n",
    "############### Slice Columns\n",
    "random_cols_one = list(range(X_train_onehot.shape[1]))\n",
    "np.random.shuffle(random_cols_one)\n",
    "\n",
    "# Do a 50/50 split\n",
    "truncation_index_one = int(X_train_onehot.shape[1]*0.5)\n",
    "\n",
    "# Get sub arrays\n",
    "X_train_onehot_1 = X_train_onehot[:, random_cols_one[:truncation_index_one]]\n",
    "X_train_onehot_2 = X_train_onehot[:, random_cols_one[truncation_index_one:]]\n",
    "\n",
    "X_test_onehot_1  = X_test_onehot[:, random_cols_one[:truncation_index_one]]\n",
    "X_test_onehot_2  = X_test_onehot[:, random_cols_one[truncation_index_one:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Models\n",
    "We used **Stacked Generalization** as the algorithm. Our Stacked Generalization\n",
    "algorithm consisted of two levels:\n",
    "1. First Level\n",
    "2. Meta Level (Second level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 First-Level layers\n",
    "\n",
    "The First Level consisted of:\n",
    "\n",
    "• Algorithms (tree-based) trained on the label encoded data, $DF_{enc}$:\n",
    " - A Random Forest algorithm, with off-the-shelf parameters\n",
    " - An XGB Regressor, which we tried to optimize using Random Search Cross Validation over the parameter space\n",
    " - A Gradient Boosting algorithm, with off-the-shelf parameterization, with a few tweaks to learning rate and depth of the trees.\n",
    " - An LGBM Regressor with off-the-shelf parameterization with smaller learning rate\n",
    "• Algorithms (linear) trained on the OneHot encoded data, $DF_{onehot}^1$ and $DF_{onehot}^2$:\n",
    " - On each of the two column partitions of our data, the same Lasso regression was applied, for a total of two models. The regularization parameter was fine tuned based on cross validation using the training data.\n",
    "\n",
    "\n",
    "The reason for the two types of algorithms mentioned above is the following: tree based algorithms\n",
    "perform better on non-dummy variables, while linear models perform better with dummy-encoded\n",
    "variables. Since there are disadvantages to both types of encoding with respect to training models, we\n",
    "decided to control for both, and thus increase the diversity of our model. The predictions from all the\n",
    "models in the 1st level were then stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1st level models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lejvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5822486847337132, tolerance: 0.23712723665948496\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "############ Declare Models 1st level\n",
    "print('Training 1st level models')\n",
    "\n",
    "### XGB hyperparams\n",
    "# Hyperparameters:\n",
    "learner = 'gbtree' #could be set to gblinear, gbtree, dart\n",
    "eval_metric_chosen = 'rmse'\n",
    "depth = 8\n",
    "estimators = 37 # number of trees\n",
    "subsamples = 0.9598482974568515 #percentage of samples used per tree \n",
    "gamma = 3.3681575402083705\n",
    "rate = 0.19662648468513327\n",
    "pct_features = 0.9236631906984126 #percentage of features used per tree\n",
    "alpha_val = 128.2243404673071 #L1 regularization on leaf weights\n",
    "\n",
    "### GB hyperparams\n",
    "gb_depth = 8\n",
    "gb_estimators = 50\n",
    "gb_rate = .4\n",
    "\n",
    "\n",
    "### Models\n",
    "tree_models = [\n",
    "    \n",
    "    RandomForestRegressor(n_estimators=100, max_depth=24, max_features=75, random_state=0),\n",
    "        \n",
    "    xgb.XGBRegressor(booster = learner, objective ='reg:squarederror', \n",
    "                          colsample_bytree = pct_features, \n",
    "                          learning_rate = rate,\n",
    "                          max_depth = depth, \n",
    "                          alpha = alpha_val, \n",
    "                          min_child_weight = 30.780029105065292,\n",
    "                          n_estimators = estimators,\n",
    "                          eval_metric = eval_metric_chosen,\n",
    "                          subsample = subsamples,\n",
    "                          random_state=0),\n",
    "    \n",
    "    GradientBoostingRegressor(n_estimators=gb_estimators,\n",
    "                                   learning_rate=gb_rate,\n",
    "                                   max_depth=gb_depth,\n",
    "                                   random_state=0, loss='ls'),\n",
    "    \n",
    "    lgb.LGBMRegressor(boosting_type = 'gbdt',  num_leaves=31, \n",
    "                          max_depth= - 1, learning_rate=0.1, \n",
    "                          n_estimators=100, subsample_for_bin=200000, \n",
    "                          objective=None, class_weight=None, \n",
    "                          min_split_gain=0.0, min_child_weight=0.001, \n",
    "                          min_child_samples=20, subsample=1.0, subsample_freq=0, \n",
    "                          colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, \n",
    "                          random_state=0, n_jobs=- 1, silent=True, \n",
    "                          importance_type='split')\n",
    "]\n",
    "\n",
    "models_dummy = [\n",
    "    Lasso(alpha = 8e-6, normalize = True, max_iter = 1000, tol = 1e-5)\n",
    "    \n",
    "        \n",
    "]\n",
    "\n",
    "\n",
    "############## Run 1st level Models\n",
    "# Train Encoded\n",
    "S_train_encoded, S_test_encoded = stacking(tree_models,                   \n",
    "                           X_train_encoded, np.ravel(y_train), X_test_encoded,   \n",
    "                           regression=True, \n",
    "     \n",
    "                           mode='oof_pred_bag', \n",
    "            \n",
    "                           metric = metrics.mean_squared_error, \n",
    "    \n",
    "                           n_folds=4, \n",
    "\n",
    "                           shuffle=True,  \n",
    "            \n",
    "                           random_state=0,    \n",
    "         \n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "## Train One Hot: Set 1\n",
    "S_train_onehot_1, S_test_onehot_1 = stacking(models_dummy,                   \n",
    "                           X_train_onehot_1, np.ravel(y_train), X_test_onehot_1,   \n",
    "                           regression=True, \n",
    "     \n",
    "                           mode='oof_pred_bag', \n",
    "            \n",
    "                           metric = metrics.mean_squared_error, \n",
    "    \n",
    "                           n_folds=4, \n",
    "            \n",
    "                           shuffle=True,  \n",
    "            \n",
    "                           random_state=0,    \n",
    "         \n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "## Train One Hot: Set 2\n",
    "S_train_onehot_2, S_test_onehot_2 = stacking(models_dummy,                   \n",
    "                           X_train_onehot_2, np.ravel(y_train), X_test_onehot_2,   \n",
    "                           regression=True, \n",
    "     \n",
    "                           mode='oof_pred_bag', \n",
    "            \n",
    "                           metric = metrics.mean_squared_error, \n",
    "    \n",
    "                           n_folds=4, \n",
    "            \n",
    "                           shuffle=True,  \n",
    "            \n",
    "                           random_state=0,    \n",
    "         \n",
    "                           verbose=0)\n",
    "\n",
    "# Stack predictions from 1st level models\n",
    "S_train = np.hstack((S_train_encoded, S_train_onehot_1, S_train_onehot_2))\n",
    "S_test = np.hstack((S_test_encoded, S_test_onehot_1, S_test_onehot_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Meta-Level layer\n",
    "\n",
    "The stacked predictions from the first layer were appended to our data again. In this layer, we turned all categorical variables into strings, and tried to spoil CATBOOST algorithm’s ability to handle categorical variables. The dataset we fed into CATBOOST consisted of the original data (with string\n",
    "categorical variables) and the six columns coming from the 1st level stacked predictions. We used\n",
    "an off-the-shelf parameterization, and fine tuned the number of iterations, the learning rate, and the\n",
    "depth of the algorithm. We appended our data to this layer at the hopes that CATBOOST would capture\n",
    "some features of the categorical nature of our data that label encoding and OneHot encoding may have\n",
    "missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Meta Model on stacked predictions from 1st level and the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lejvi\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Meta level\n",
    "print('Training Meta Model on stacked predictions from 1st level and the data')\n",
    "\n",
    "# Cat Boost implementation: requires conda install -c conda-forge catboost\n",
    "\n",
    "#########################################################################\n",
    "################### Making data categorical first to feed into cat boost\n",
    "Train_cat = Train\n",
    "Test_cat  = Test\n",
    "\n",
    "Train_cat = Train_cat[vars_all_encoded +['y']]   \n",
    "Test_cat = Test_cat[vars_all_encoded +['y']]  \n",
    "\n",
    "for col in vars_dummy_encoded:\n",
    "    Train_cat.loc[:,col] = Train[col].astype(str)\n",
    "    Test_cat.loc[:,col] = Test[col].astype(str)\n",
    "    \n",
    "# Merge in embedded occ and occly\n",
    "Train_cat = pd.concat([Train_cat, X_embed_train], axis = 1)\n",
    "Test_cat = pd.concat([Test_cat, X_embed_test], axis = 1)\n",
    "\n",
    "X_train_cat = Train_cat.drop(\"y\", axis = 1)\n",
    "y_train_cat = Train_cat[\"y\"]    \n",
    "    \n",
    "X_test_cat = Test_cat.drop(\"y\", axis = 1)\n",
    "y_test_cat = Test_cat[\"y\"]      \n",
    "\n",
    "\n",
    "############# Append original data to predictions from 1st level\n",
    "S_train_append = pd.DataFrame(S_train, columns = ['predictor' + str(x) for x in range(1,S_train.shape[1]+1)])\n",
    "S_test_append = pd.DataFrame(S_test, columns = ['predictor' + str(x) for x in range(1,S_train.shape[1]+1)])\n",
    "\n",
    "X_train_cat_append = pd.concat((X_train_cat,S_train_append), axis = 1)\n",
    "X_test_cat_append = pd.concat((X_test_cat,S_test_append), axis = 1)\n",
    "\n",
    "#####################################################################\n",
    "############## Meta Algorithm: CatBoost\n",
    "cb = CatBoostRegressor(loss_function = 'RMSE', random_seed= 0, iterations=3000, learning_rate= .01, depth= 10, verbose = 0)\n",
    "\n",
    "cb.fit(X_train_cat_append,y_train_cat)\n",
    "\n",
    "y_pred_app = cb.predict(X_test_cat_append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Results\n",
    "\n",
    "The algorithm was trained\n",
    "using only the training sample with log wages as the target variable. The results show the error on\n",
    "the test sample using the three metrics required for the competition:\n",
    "\n",
    "**RMS-log:** 0.4774074678408843 \n",
    "\n",
    "**RMS-level:** 22488.752597078223\n",
    "\n",
    "**AbsDev-log:** 3.165844131187269\n",
    "\n",
    "The RMS-log for the Mincer Regression is 0.74 for our data. Our algorithm thus improves RMS-log by at least $35\\%$ relative to the Mincer Regression model of earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data is: 0.4774074678408843\n",
      "RMSE on test data using levels is: 22488.752597078223\n",
      "Absolute Deviation on test data is: 3.165844131187269\n"
     ]
    }
   ],
   "source": [
    "print('RMSE on test data is:', np.sqrt(metrics.mean_squared_error(np.ravel(y_test), y_pred_app)))\n",
    "\n",
    "print('RMSE on test data using levels is:', np.sqrt(metrics.mean_squared_error(np.ravel(np.exp(y_test)), np.exp(y_pred_app))))\n",
    "\n",
    "print('Absolute Deviation on test data is:', np.max(np.abs(np.ravel(y_test) - y_pred_app)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
